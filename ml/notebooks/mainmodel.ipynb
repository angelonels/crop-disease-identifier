{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61e3000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mathematical engine bound to: mps\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import timm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "# The NVIDIA hardware handshake\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Mathematical engine bound to: {device}\")\n",
    "\n",
    "# Define our established data topography\n",
    "DATASET_DIR = \"../dataset/PlantVillage\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b29d82ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taxonomy contract sealed. Total classes modeled: 38\n",
      "Sample of the taxonomy dictionary:\n",
      "[\n",
      "  [\n",
      "    \"apple\",\n",
      "    {\n",
      "      \"apple_scab\": 0,\n",
      "      \"black_rot\": 1,\n",
      "      \"cedar_apple_rust\": 2,\n",
      "      \"healthy\": 3\n",
      "    }\n",
      "  ],\n",
      "  [\n",
      "    \"blueberry\",\n",
      "    {\n",
      "      \"healthy\": 4\n",
      "    }\n",
      "  ]\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "dataset_full = datasets.ImageFolder(root=DATASET_DIR)\n",
    "classes = dataset_full.classes\n",
    "\n",
    "manifest = {}\n",
    "for idx, class_name in enumerate(classes):\n",
    "    # PlantVillage nomenclature is strictly formatted: \"Species___Disease\"\n",
    "    parts = class_name.split(\"___\")\n",
    "    species = parts[0].lower()\n",
    "    disease = parts[1].lower() if len(parts) > 1 else \"unknown\"\n",
    "    \n",
    "    if species not in manifest:\n",
    "        manifest[species] = {}\n",
    "    \n",
    "    manifest[species][disease] = idx\n",
    "\n",
    "# Crystallize the contract to disk for the Node.js backend\n",
    "with open(\"plant_manifest.json\", \"w\") as f:\n",
    "    json.dump(manifest, f, indent=4)\n",
    "\n",
    "print(f\"Taxonomy contract sealed. Total classes modeled: {len(classes)}\")\n",
    "print(\"Sample of the taxonomy dictionary:\")\n",
    "print(json.dumps(list(manifest.items())[0:2], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f001dff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory intake choked. Physical batch size locked to 2.\n"
     ]
    }
   ],
   "source": [
    "standard_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "dataset_full = datasets.ImageFolder(root=DATASET_DIR, transform=standard_transform)\n",
    "\n",
    "# Rigorous Stratification: We extract the targets to ensure the 80/20 split \n",
    "# respects the inherent class imbalance of the dataset.\n",
    "targets = dataset_full.targets\n",
    "train_idx, val_idx = train_test_split(\n",
    "    np.arange(len(targets)), \n",
    "    test_size=0.2, \n",
    "    stratify=targets, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "train_data = torch.utils.data.Subset(dataset_full, train_idx)\n",
    "val_data = torch.utils.data.Subset(dataset_full, val_idx)\n",
    "\n",
    "# The VRAM is capacious. Revert to an optimal batch size.\n",
    "OPTIMAL_BATCH_SIZE = 32\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=OPTIMAL_BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_data, batch_size=OPTIMAL_BATCH_SIZE, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac631678",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1cd7229c11f4dff8d9d9bd660f35fa4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/119M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Swin V2 Architecture initialized. Final layer adapted to output 38 logits.\n"
     ]
    }
   ],
   "source": [
    "num_classes = len(dataset_full.classes)\n",
    "\n",
    "\n",
    "model = timm.create_model('swinv2_tiny_window16_256', pretrained=True, num_classes=num_classes)\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"Swin V2 Architecture initialized. Final layer adapted to output {num_classes} logits.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3995c443",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n",
    "        \n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        return focal_loss.sum()\n",
    "\n",
    "criterion = FocalLoss(gamma=2.0)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f561466",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "MPS backend out of memory (MPS allocated: 9.06 GiB, other allocations: 4.59 MiB, max allowed: 9.07 GiB). Tried to allocate 12.00 MiB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# PRONG 2: Automatic Mixed Precision (bfloat16)\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# This explicitly halves the memory footprint of the massive QKV attention matrices\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.autocast(device_type=\u001b[33m\"\u001b[39m\u001b[33mmps\u001b[39m\u001b[33m\"\u001b[39m, dtype=torch.bfloat16):\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m     outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m     loss = criterion(outputs, labels) / accumulation_steps\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# Backward pass remains outside autocast to maintain gradient precision\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/crop-disease-identifier/ml/venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1776\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1776\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/crop-disease-identifier/ml/venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1787\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1786\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1787\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1789\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1790\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/crop-disease-identifier/ml/venv/lib/python3.13/site-packages/timm/models/swin_transformer_v2.py:612\u001b[39m, in \u001b[36mSwinTransformerV2.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    611\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m--> \u001b[39m\u001b[32m612\u001b[39m     x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    613\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.forward_head(x)\n\u001b[32m    614\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/crop-disease-identifier/ml/venv/lib/python3.13/site-packages/timm/models/swin_transformer_v2.py:604\u001b[39m, in \u001b[36mSwinTransformerV2.forward_features\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    602\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward_features\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m    603\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.patch_embed(x)\n\u001b[32m--> \u001b[39m\u001b[32m604\u001b[39m     x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlayers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    605\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.norm(x)\n\u001b[32m    606\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/crop-disease-identifier/ml/venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1776\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1776\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/crop-disease-identifier/ml/venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1787\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1786\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1787\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1789\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1790\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/crop-disease-identifier/ml/venv/lib/python3.13/site-packages/torch/nn/modules/container.py:253\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    249\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    250\u001b[39m \u001b[33;03mRuns the forward pass.\u001b[39;00m\n\u001b[32m    251\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m253\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/crop-disease-identifier/ml/venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1776\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1776\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/crop-disease-identifier/ml/venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1787\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1786\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1787\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1789\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1790\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/crop-disease-identifier/ml/venv/lib/python3.13/site-packages/timm/models/swin_transformer_v2.py:437\u001b[39m, in \u001b[36mSwinTransformerV2Stage.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    435\u001b[39m         x = checkpoint.checkpoint(blk, x)\n\u001b[32m    436\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m437\u001b[39m         x = \u001b[43mblk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    438\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/crop-disease-identifier/ml/venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1776\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1776\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/crop-disease-identifier/ml/venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1787\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1786\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1787\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1789\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1790\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/crop-disease-identifier/ml/venv/lib/python3.13/site-packages/timm/models/swin_transformer_v2.py:322\u001b[39m, in \u001b[36mSwinTransformerV2Block.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    320\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m    321\u001b[39m     B, H, W, C = x.shape\n\u001b[32m--> \u001b[39m\u001b[32m322\u001b[39m     x = x + \u001b[38;5;28mself\u001b[39m.drop_path1(\u001b[38;5;28mself\u001b[39m.norm1(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[32m    323\u001b[39m     x = x.reshape(B, -\u001b[32m1\u001b[39m, C)\n\u001b[32m    324\u001b[39m     x = x + \u001b[38;5;28mself\u001b[39m.drop_path2(\u001b[38;5;28mself\u001b[39m.norm2(\u001b[38;5;28mself\u001b[39m.mlp(x)))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/crop-disease-identifier/ml/venv/lib/python3.13/site-packages/timm/models/swin_transformer_v2.py:307\u001b[39m, in \u001b[36mSwinTransformerV2Block._attn\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    304\u001b[39m x_windows = x_windows.view(-\u001b[32m1\u001b[39m, \u001b[38;5;28mself\u001b[39m.window_area, C)  \u001b[38;5;66;03m# nW*B, window_size*window_size, C\u001b[39;00m\n\u001b[32m    306\u001b[39m \u001b[38;5;66;03m# W-MSA/SW-MSA\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m307\u001b[39m attn_windows = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_windows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# nW*B, window_size*window_size, C\u001b[39;00m\n\u001b[32m    309\u001b[39m \u001b[38;5;66;03m# merge windows\u001b[39;00m\n\u001b[32m    310\u001b[39m attn_windows = attn_windows.view(-\u001b[32m1\u001b[39m, \u001b[38;5;28mself\u001b[39m.window_size[\u001b[32m0\u001b[39m], \u001b[38;5;28mself\u001b[39m.window_size[\u001b[32m1\u001b[39m], C)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/crop-disease-identifier/ml/venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1776\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1776\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/crop-disease-identifier/ml/venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1787\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1786\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1787\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1789\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1790\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/crop-disease-identifier/ml/venv/lib/python3.13/site-packages/timm/models/swin_transformer_v2.py:176\u001b[39m, in \u001b[36mWindowAttention.forward\u001b[39m\u001b[34m(self, x, mask)\u001b[39m\n\u001b[32m    174\u001b[39m relative_position_bias = relative_position_bias.permute(\u001b[32m2\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m).contiguous()  \u001b[38;5;66;03m# nH, Wh*Ww, Wh*Ww\u001b[39;00m\n\u001b[32m    175\u001b[39m relative_position_bias = \u001b[32m16\u001b[39m * torch.sigmoid(relative_position_bias)\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m attn = \u001b[43mattn\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mrelative_position_bias\u001b[49m\u001b[43m.\u001b[49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    179\u001b[39m     num_win = mask.shape[\u001b[32m0\u001b[39m]\n",
      "\u001b[31mRuntimeError\u001b[39m: MPS backend out of memory (MPS allocated: 9.06 GiB, other allocations: 4.59 MiB, max allowed: 9.07 GiB). Tried to allocate 12.00 MiB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure)."
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "# We no longer need extreme accumulation because our physical batch size is 32\n",
    "accumulation_steps = 1 \n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    optimizer.zero_grad() \n",
    "    \n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # Native CUDA mixed precision\n",
    "        with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "        \n",
    "        loss.backward() \n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad() \n",
    "            \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Trajectory Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13eed4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in val_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # Argmax collapses the 38 logits into a single integer prediction\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        \n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "acc = accuracy_score(all_labels, all_preds)\n",
    "f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "\n",
    "print(f\"Validation Accuracy: {acc * 100:.2f}%\")\n",
    "print(f\"Macro F1-Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3560e99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "# We construct a tensor of shape [Batch_Size, Channels, Height, Width]\n",
    "dummy_input = torch.randn(1, 3, 256, 256, device=device)\n",
    "onnx_path = \"swin_v2_mvp.onnx\"\n",
    "\n",
    "torch.onnx.export(\n",
    "    model, \n",
    "    dummy_input, \n",
    "    onnx_path,\n",
    "    export_params=True,\n",
    "    opset_version=14,\n",
    "    do_constant_folding=True,\n",
    "    input_names=['input'],\n",
    "    output_names=['output'],\n",
    "    dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}}\n",
    ")\n",
    "\n",
    "print(f\"Architecture crystallized. Artifact {onnx_path} generated.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6716f413",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from tqdm import tqdm # For a sleek progress bar\n",
    "\n",
    "quantized_model_path = \"swin_v2_mvp_quantized.onnx\"\n",
    "\n",
    "print(\"1. Initializing the ONNX Runtime Execution Provider...\")\n",
    "# We load the quantized brain. We explicitly use the CPU provider because \n",
    "# quantized integer operations are hyper-optimized for CPU execution.\n",
    "session = ort.InferenceSession(quantized_model_path, providers=['CPUExecutionProvider'])\n",
    "\n",
    "# Dynamically extract the exact input tensor name (e.g., \"x\")\n",
    "input_name = session.get_inputs()[0].name\n",
    "print(f\"   Engine ready. Awaiting tensors at input node: '{input_name}'\")\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "print(\"\\n2. Commencing inference audit across the validation dataset...\")\n",
    "# We use the val_loader we already constructed earlier in the notebook\n",
    "for inputs, labels in tqdm(val_loader, desc=\"Quantized Inference\"):\n",
    "    # The ONNX engine speaks strictly in NumPy arrays, not PyTorch tensors.\n",
    "    # We must pull the tensors off the GPU (if they are there) and convert them.\n",
    "    inputs_np = inputs.cpu().numpy()\n",
    "    \n",
    "    # Execute the forward pass through the quantized graph\n",
    "    outputs = session.run(None, {input_name: inputs_np})[0]\n",
    "    \n",
    "    # The output is a matrix of raw logits. We take the argmax to find the winning class.\n",
    "    preds = np.argmax(outputs, axis=1)\n",
    "    \n",
    "    all_preds.extend(preds)\n",
    "    all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# 3. The Final Verdict\n",
    "acc = accuracy_score(all_labels, all_preds)\n",
    "f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "\n",
    "print(\"\\n--- The Quantization Audit ---\")\n",
    "print(f\"Quantized Accuracy: {acc * 100:.2f}%\")\n",
    "print(f\"Quantized Macro-F1: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddccc6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "quantized_model_path = \"swin_v2_mvp_quantized.onnx\"\n",
    "\n",
    "print(\"1. Initializing the ONNX Runtime Execution Provider...\")\n",
    "session = ort.InferenceSession(quantized_model_path, providers=['CPUExecutionProvider'])\n",
    "input_name = session.get_inputs()[0].name\n",
    "print(f\"   Engine ready. Awaiting tensors strictly shaped [1, 3, 256, 256] at node: '{input_name}'\")\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "print(\"\\n2. Commencing inference audit across the validation dataset...\")\n",
    "for inputs, labels in tqdm(val_loader, desc=\"Quantized Inference\"):\n",
    "    inputs_np = inputs.cpu().numpy()\n",
    "    labels_np = labels.cpu().numpy()\n",
    "    \n",
    "    # Our ONNX model is hardcoded to accept a batch size of EXACTLY 1.\n",
    "    # We must slice our batch of 32 into 32 individual inferences.\n",
    "    for i in range(inputs_np.shape[0]):\n",
    "        # Slicing i:i+1 preserves the 4D tensor shape: [1, 3, 256, 256]\n",
    "        single_leaf_tensor = inputs_np[i:i+1] \n",
    "        \n",
    "        # Execute the forward pass\n",
    "        outputs = session.run(None, {input_name: single_leaf_tensor})[0]\n",
    "        \n",
    "        # Extract the winning class prediction\n",
    "        pred = np.argmax(outputs, axis=1)[0]\n",
    "        \n",
    "        all_preds.append(pred)\n",
    "        all_labels.append(labels_np[i])\n",
    "\n",
    "# 3. The Final Verdict\n",
    "acc = accuracy_score(all_labels, all_preds)\n",
    "f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "\n",
    "print(\"\\n--- The Quantization Audit ---\")\n",
    "print(f\"Quantized Accuracy: {acc * 100:.2f}%\")\n",
    "print(f\"Quantized Macro-F1: {f1:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
